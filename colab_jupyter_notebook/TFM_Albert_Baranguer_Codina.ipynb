{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFM_Albert_Baranguer_Codina.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1aeN_N9KLElhA_zN4Q9KSWrmN58mCH0zS","authorship_tag":"ABX9TyMB8NRtUCAno+PEfmVSC0Qg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ia8RRIkKzPR6","colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"status":"ok","timestamp":1634586152129,"user_tz":-120,"elapsed":11139,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}},"outputId":"7fce5814-6353-42f6-88b0-14f2054f6130"},"source":["# TFM Albert Baranguer i Codina\n","# Entrenament d'una xarxa ResNet18 per a la classificació del dataset HAM10000\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","git_repo = 'https://github.com/abaranguer/uoc_tfm'\n","\n","!pip install colab_ssh --upgrade --quiet\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","\n","password='PasswordCloudflaredTfm202122'\n","launch_ssh_cloudflared(password)\n","\n","init_git_cloudflared(repository_url=git_repo + \".git\",\n","         personal_token=\"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\", \n","         branch=\"main\",\n","         email=\"abaranguer@gmail.com\",\n","         username=\"abaranguer\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"display_data","data":{"text/html":["<style>\n","*{\n","\toutline:none;\n","}\n","code{\n","\tdisplay:inline-block;\n","\tpadding:5px 10px;\n","\tbackground: #444;\n","\tborder-radius: 4px;\n","\twhite-space: pre-wrap;\n","\tposition:relative;\n","\tcolor:white;\n","}\n",".copy-code-button{\n","\tfloat:right;\n","\tbackground:#333;\n","\tcolor:white;\n","\tborder: none;\n","\tmargin: 0 0 0 10px;\n","\tcursor: pointer;\n","}\n","p, li{\n","\tmax-width:700px;\n","}\n",".choices{\n","\tdisplay:flex;\n","\tflex: 1 0 auto;\n","}\n",".choice-section{\n","\tborder:solid 1px #555;\n","\tborder-radius: 4px;\n","\tmin-width:300px;\n","\tmargin: 10px 15px 0 0;\n","\tpadding: 0 15px 15px 15px ;\n","}\n",".button{\n","\tpadding: 10px 15px;\n","\tbackground:#333;\n","\tborder-radius: 4px;\n","\tborder:solid 1px #555;\n","\tcolor:white;\n","\tfont-weight:bold;\n","\tcursor:pointer;\n","}\n",".pill{\n","\tpadding:2px 4px;\n","\tborder-radius: 100px;\n","\tbackground-color:#e65858;\n","\tfont-size:12px;\n","\tfont-weight:bold;\n","\tmargin: 0 15px;\n","\tcolor:white;\n","}\n","</style>\n","<details class=\"choice-section\">\n","\t<summary style=\"cursor:pointer\">\n","\t\t<h3 style=\"display:inline-block;margin-top:15px\">⚙️ Client machine configuration<span class=\"pill\">Required</span></h3>\n","\t</summary>\n","\t<p>Don't worry, you only have to do this <b>once per client machine</b>.</p>\n","\t<ol>\n","\t\t<li>Download <a href=\"https://developers.cloudflare.com/argo-tunnel/getting-started/installation\">Cloudflared (Argo Tunnel)</a>, then copy the absolute path of the cloudflare binary</li>\n","\t\t<li>Now, you have to append the following to your SSH config file (usually under ~/.ssh/config), and make sure you replace the placeholder with the path you copied in Step 1:</li>\n","\t</ol>\n","\t<code>Host *.trycloudflare.com\n","\tHostName %h\n","\tUser root\n","\tPort 22\n","\tProxyCommand &ltPUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&gt access ssh --hostname %h\n","\t</code>\n","</details>\n","<div class=\"choices\">\n","\t<div class=\"choice-section\">\n","\t\t<h4>SSH Terminal</h4>\n","\t\t<p>To connect using your terminal, type this command:</p>\n","\t\t<code>ssh tags-abilities-earthquake-pressed.trycloudflare.com</code>\n","\t</div>\n","\t<div class=\"choice-section\">\n","\t\t<h4>VSCode Remote SSH</h4>\n","\t\t<p>You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:</p>\n","\t\t<code>tags-abilities-earthquake-pressed.trycloudflare.com</code>\n","\t</div>\n","</div>\n","\n","<script>\n","// Copy any string\n","function fallbackCopyTextToClipboard(text) {\n","  var textArea = document.createElement(\"textarea\");\n","  textArea.value = text;\n","  \n","  // Avoid scrolling to bottom\n","  textArea.style.top = \"0\";\n","  textArea.style.left = \"0\";\n","  textArea.style.position = \"fixed\";\n","\n","  document.body.appendChild(textArea);\n","  textArea.focus();\n","  textArea.select();\n","\n","  try {\n","    var successful = document.execCommand('copy');\n","    var msg = successful ? 'successful' : 'unsuccessful';\n","    console.log('Fallback: Copying text command was ' + msg);\n","  } catch (err) {\n","    console.error('Fallback: Oops, unable to copy', err);\n","  }\n","\n","  document.body.removeChild(textArea);\n","}\n","\n","// Show the copy button with every code tag\n","document.querySelectorAll('code').forEach(function (codeBlock) {\n","\tconst codeToCopy= codeBlock.innerText;\n","\tvar pre = document.createElement('pre');\n","\tpre.innerText = codeToCopy;\n","    var button = document.createElement('button');\n","    button.className = 'copy-code-button';\n","    button.type = 'button';\n","    button.innerText = 'Copy';\n","\tbutton.onclick = function(){\n","\t\tfallbackCopyTextToClipboard(codeToCopy);\n","\t\tbutton.innerText = 'Copied'\n","\t\tsetTimeout(()=>{\n","\t\t\tbutton.innerText = 'Copy'\n","\t\t},2000)\n","\t}\n","\tcodeBlock.children = pre;\n","\tcodeBlock.prepend(button)\n","});\n","</script>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully cloned the repository in ./uoc_tfm\n"]}]},{"cell_type":"code","metadata":{"id":"z6Ze4B5J8VGa","executionInfo":{"status":"ok","timestamp":1634586538748,"user_tz":-120,"elapsed":285,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["# dx (classes) - classe to int and viceversa\n","\n","dx_to_int = {\n","    'akiec': 0,\n","    'bcc': 1,\n","    'bkl': 2,\n","    'df': 3,\n","    'nv': 4,\n","    'mel': 5,\n","    'vasc': 6\n","}\n","\n","int_to_dx = [\n","     'akiec',\n","     'bcc',\n","     'bkl',\n","     'df',\n","     'nv',\n","     'mel',\n","     'vasc'\n","]\n","\n","dx_to_description = {\n","    'akiec': 'Actinic Keratoses and Intraepithelial Carcinoma',\n","    'bcc': 'Basal cell carcinoma',\n","    'bkl': '\"Benign keratosis\"',\n","    'df': 'Dermatofibroma',\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'Melanoma',\n","    'vasc': 'Vascular skin lesions'\n","}\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"IceGVZsY80c2","executionInfo":{"status":"ok","timestamp":1634586761081,"user_tz":-120,"elapsed":272,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["# dataset analyzer\n","\n","import pandas\n","import time\n","\n","\n","class Ham10000DatasetAnalyzer:\n","    def __init__(self):\n","        self.path = None\n","        self.df = None\n","        self.num_of_images = 0\n","        self.dataset_classes = 0\n","        self.dataset_classes_counts = None\n","\n","    def analyze_path(self, path):\n","        self.path = path\n","        self.df = pandas.read_csv(path)\n","        self.analyze()\n","\n","    def analyze_dataframe(self, df):\n","        self.path = None\n","        self.df = df\n","        self.analyze()\n","\n","    def analyze(self):\n","        self.num_of_images = len(self.df['dx'])\n","        self.dataset_classes = self.df['dx'].unique()\n","        self.dataset_classes_counts = self.df['dx'].value_counts()\n","\n","    def metadata(self):\n","        return self.num_of_images, self.dataset_classes, self.dataset_classes_counts\n","\n","    def show(self, title):\n","        print(f'---- Analyzer. {title} ----\\n')\n","        print(f'num of images: {self.num_of_images}')\n","        print(f'num of classes: {self.dataset_classes}')\n","        for dataset_classe_count in enumerate(self.dataset_classes_counts):\n","            print(\n","                f'\\tclasse: \"{self.dataset_classes[dataset_classe_count[0]]}\"; num of images: {dataset_classe_count[1]};{(100.0 * dataset_classe_count[1] / self.num_of_images): .2f} % of the dataset.')\n","        print('------------------------')\n","\n","    def save_dataframe(self, data_frame, filename):\n","        path = '/content/drive/MyDrive/UOC-TFM/dataframes/'\n","        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n","        filename = path + timestamp + '_' + filename\n","        data_frame.to_pickle(filename)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQEnowD38--g","executionInfo":{"status":"ok","timestamp":1634586919458,"user_tz":-120,"elapsed":293,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["# dataset splitter\n","\n","import numpy as np\n","import pandas\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","class Ham10000DatasetSplitter:\n","    def __init__(self, dataset_metadata_path, dataset_images_path,\n","                 percent_val=0.15, percent_test=0.15,\n","                 BATCH_SIZE=100, VAL_BATCH_SIZE=20, TEST_BATCH_SIZE=20):\n","        np.random.seed(0)\n","        analyzer = Ham10000DatasetAnalyzer()\n","        analyzer.analyze_path(dataset_metadata_path)\n","        analyzer.show('FULL DATASET')\n","\n","        df = pandas.read_csv(dataset_metadata_path)\n","        percent_validation = percent_val + percent_test\n","        self.train_set, val_test_set = train_test_split(df, test_size=percent_validation)\n","        percent_test_validation = percent_test / percent_validation\n","        self.validation_set, self.test_set = train_test_split(val_test_set, test_size=percent_test_validation)\n","\n","        analyzer.analyze_dataframe(self.train_set)\n","        analyzer.show('TRAIN SET')\n","        analyzer.save_dataframe(self.train_set, 'dataframe_train_set.pkl')\n","\n","        analyzer.analyze_dataframe(self.validation_set)\n","        analyzer.show('VALIDATION SET')\n","        analyzer.save_dataframe(self.train_set, 'dataframe_validation_set.pkl')\n","\n","        analyzer.analyze_dataframe(self.test_set)\n","        analyzer.show('TEST SET')\n","        analyzer.save_dataframe(self.train_set, 'dataframe_test_set.pkl')\n","\n","        self.data_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","        '''\n","        '# training data\n","        train_data_transform = transforms.Compose([\n","            transforms.Resize(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        '''\n","\n","        self.train_dataset = Ham10000Dataset(self.train_set, dataset_images_path, self.data_transform)\n","        self.validation_dataset = Ham10000Dataset(self.validation_set, dataset_images_path, self.data_transform)\n","        self.test_dataset = Ham10000Dataset(self.test_set, dataset_images_path, self.data_transform)\n","\n","        self.train_dataloader = DataLoader(\n","            self.train_dataset,\n","            batch_size=BATCH_SIZE,\n","            shuffle=True\n","        )\n","\n","        self.validation_dataloader = DataLoader(\n","            self.validation_dataset,\n","            batch_size=VAL_BATCH_SIZE,\n","            shuffle=True\n","        )\n","\n","        self.test_dataloader = DataLoader(\n","            self.test_dataset,\n","            batch_size=TEST_BATCH_SIZE,\n","            shuffle=True\n","        )"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHLExAiI_Pvs","executionInfo":{"status":"ok","timestamp":1634587676281,"user_tz":-120,"elapsed":295,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["# HAM10000 Dataset\n"," \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas\n","import torchvision\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","\n","\n","class Ham10000Dataset(Dataset):\n","    def __init__(self, csv, img_folder, transform):\n","        self.csv = csv\n","        self.transform = transform\n","        self.img_folder = img_folder\n","        self.image_names = self.csv[:]['image_id']\n","        self.labels = np.array(\n","            self.csv.drop(['lesion_id', 'dx_type', 'age', 'sex', 'localization', 'dataset'], axis=1))\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, index):\n","        img_path = self.img_folder + self.image_names.iloc[index] + '.jpg'\n","        image = Image.open(img_path).convert('RGB')\n","        image = self.transform(image)\n","        targets = self.labels[index]\n","        return {'image': image,\n","                'image_id': targets[0],\n","                'dx': targets[1],\n","                'label': dx_to_int[targets[1]]}\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"bI-KCPfwBiz8","executionInfo":{"status":"ok","timestamp":1634588019758,"user_tz":-120,"elapsed":313,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["#Resnet18 trainer\n","\n","import time\n","import torch.optim\n","import torchvision.models as models\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import SGD\n","\n","\n","class Ham10000ResNet18Trainer:\n","\n","    def __init__(self, train_dataloader, model, epochs=5):\n","        self.train_dataloader = train_dataloader\n","        self.model = model\n","        self.epochs = epochs\n","        self.loss = None\n","        self.optimizer = None\n","        self.which_device = \"\"\n","\n","    def run_training(self):\n","        self.loss = CrossEntropyLoss()\n","        self.optimizer = SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n","\n","        # select device (GPU or CPU)\n","        self.which_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","        print(f'using {self.which_device} device')\n","        device = torch.device(self.which_device)\n","\n","        for epoch in range(self.epochs):  # loop over the dataset multiple times\n","            running_loss = 0.0\n","\n","            for i, images in enumerate(self.train_dataloader, 0):\n","                inputs = images['image']\n","                labels = images['label']\n","\n","                self.optimizer.zero_grad()\n","\n","                outputs = self.model(inputs)\n","                loss_current = self.loss(outputs, labels)\n","                loss_current.backward()\n","                self.optimizer.step()\n","\n","                running_loss += loss_current.item()\n","                print(f'epoch: {epoch}; i : {i}')\n","                if i % 100 == 99:  # print every 100 mini-batches\n","                    print('[%d, %5d] loss: %.3f' %\n","                          (epoch + 1, i + 1, running_loss / 100))\n","                    running_loss = 0.0\n","\n","        print('Finished Training')\n","\n","        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n","        trained_model_filename = timestamp + '_ham10000_trained_model.pth'\n","        torch.save(self.model.state_dict(), trained_model_filename)\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODDS9XcKB5In","executionInfo":{"status":"ok","timestamp":1634587925716,"user_tz":-120,"elapsed":265,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["#resnet18 predictor\n","\n","import numpy as np\n","import pandas\n","import torch.optim\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","class Ham10000ResNet18Predictor:\n","    def __init__(self, model, test_dataloader):\n","        self.model = model\n","        self.test_dataloader = test_dataloader\n","\n","    def run_predictor(self):\n","        images = next(iter(self.test_dataloader))\n","\n","        with torch.no_grad():\n","            images_as_tensors = images['image']\n","            outputs = model(images_as_tensors)\n","            _, predicted = torch.max(outputs, 1)\n","\n","        print('Predicted: ', ' '.join('%5s' % int_to_dx[int(predicted[j])] for j in range(len(predicted))))\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqHR82oWCJX3","executionInfo":{"status":"ok","timestamp":1634588000194,"user_tz":-120,"elapsed":306,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["#resnet18 validator\n","\n","import numpy as np\n","import pandas\n","import torch.optim\n","import torchvision\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","class Ham10000ResNet18Validator:\n","    def __init__(self, model, validation_dataloader):\n","        self.model = model\n","        self.validation_dataloader = validation_dataloader\n","        self.accuracy = 0.0\n","\n","    def run_validation(self):\n","        correct = 0\n","        total = 0\n","\n","        for i, images in enumerate(self.validation_dataloader, 0):\n","            inputs = images['image']\n","            labels = images['label']\n","\n","            print(f'batch {i}')\n","\n","            with torch.no_grad():\n","                outputs = self.model(inputs)\n","\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        self.accuracy = 100 * correct / total\n","        print(f'num of correct predicted images (True positives): {correct}')\n","        print(f'num of images : {total}')\n","        print(f'Accuracy of the network on the test images: {self.accuracy: .4f}%')\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1DynfZq-SFN","executionInfo":{"status":"ok","timestamp":1634588122445,"user_tz":-120,"elapsed":289,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}}},"source":["import time\n","\n","def log_time(message):\n","    start_time = time.strftime(\"%Y%m%d - %H%M%S\")\n","    print(f'{message} {start_time}')"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZ5qBK5tCq6X","executionInfo":{"status":"ok","timestamp":1634588388117,"user_tz":-120,"elapsed":1179,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}},"outputId":"193185ca-2960-466d-9074-f6deb134fbed"},"source":["matadata_path = '/content/drive/MyDrive/UOC-TFM/dataset/HAM10000_metadata'\n","images_path = '/content/drive/MyDrive/UOC-TFM/dataset/dataset_ham_10000/ham10000/300x225/'\n","\n","print('1 . Splits training, validation and test sets')\n","splitter = Ham10000DatasetSplitter(matadata_path, images_path)\n","train_dataloader = splitter.train_dataloader\n","validation_dataloader = splitter.validation_dataloader\n","test_dataloader = splitter.test_dataloader\n"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["1 . Splits training, validation and test sets\n","---- Analyzer. FULL DATASET ----\n","\n","num of images: 10015\n","num of classes: ['bkl' 'nv' 'df' 'mel' 'vasc' 'bcc' 'akiec']\n","\tclasse: \"bkl\"; num of images: 6705; 66.95 % of the dataset.\n","\tclasse: \"nv\"; num of images: 1113; 11.11 % of the dataset.\n","\tclasse: \"df\"; num of images: 1099; 10.97 % of the dataset.\n","\tclasse: \"mel\"; num of images: 514; 5.13 % of the dataset.\n","\tclasse: \"vasc\"; num of images: 327; 3.27 % of the dataset.\n","\tclasse: \"bcc\"; num of images: 142; 1.42 % of the dataset.\n","\tclasse: \"akiec\"; num of images: 115; 1.15 % of the dataset.\n","------------------------\n","---- Analyzer. TRAIN SET ----\n","\n","num of images: 7010\n","num of classes: ['nv' 'bkl' 'bcc' 'akiec' 'vasc' 'df' 'mel']\n","\tclasse: \"nv\"; num of images: 4693; 66.95 % of the dataset.\n","\tclasse: \"bkl\"; num of images: 784; 11.18 % of the dataset.\n","\tclasse: \"bcc\"; num of images: 775; 11.06 % of the dataset.\n","\tclasse: \"akiec\"; num of images: 350; 4.99 % of the dataset.\n","\tclasse: \"vasc\"; num of images: 234; 3.34 % of the dataset.\n","\tclasse: \"df\"; num of images: 103; 1.47 % of the dataset.\n","\tclasse: \"mel\"; num of images: 71; 1.01 % of the dataset.\n","------------------------\n","---- Analyzer. VALIDATION SET ----\n","\n","num of images: 1502\n","num of classes: ['mel' 'nv' 'akiec' 'bkl' 'vasc' 'bcc' 'df']\n","\tclasse: \"mel\"; num of images: 991; 65.98 % of the dataset.\n","\tclasse: \"nv\"; num of images: 182; 12.12 % of the dataset.\n","\tclasse: \"akiec\"; num of images: 156; 10.39 % of the dataset.\n","\tclasse: \"bkl\"; num of images: 79; 5.26 % of the dataset.\n","\tclasse: \"vasc\"; num of images: 49; 3.26 % of the dataset.\n","\tclasse: \"bcc\"; num of images: 25; 1.66 % of the dataset.\n","\tclasse: \"df\"; num of images: 20; 1.33 % of the dataset.\n","------------------------\n","---- Analyzer. TEST SET ----\n","\n","num of images: 1503\n","num of classes: ['nv' 'bcc' 'bkl' 'mel' 'akiec' 'df' 'vasc']\n","\tclasse: \"nv\"; num of images: 1021; 67.93 % of the dataset.\n","\tclasse: \"bcc\"; num of images: 159; 10.58 % of the dataset.\n","\tclasse: \"bkl\"; num of images: 156; 10.38 % of the dataset.\n","\tclasse: \"mel\"; num of images: 85; 5.66 % of the dataset.\n","\tclasse: \"akiec\"; num of images: 44; 2.93 % of the dataset.\n","\tclasse: \"df\"; num of images: 19; 1.26 % of the dataset.\n","\tclasse: \"vasc\"; num of images: 19; 1.26 % of the dataset.\n","------------------------\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H11kBV6FD5Gh","executionInfo":{"status":"ok","timestamp":1634588425172,"user_tz":-120,"elapsed":661,"user":{"displayName":"Albert Baranguer Codina","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06766720959722371664"}},"outputId":"c2144b67-2d74-4c29-bbd9-27837fd3a8d3"},"source":["print('2 - create ResNet18 model')\n","model = models.resnet18()"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2 - create ResNet18 model\n"]}]},{"cell_type":"code","metadata":{"id":"FRxFR6LBEA_M"},"source":["print('3 - train model')\n","trainer = Ham10000ResNet18Trainer(train_dataloader, model)\n","\n","log_time('\\tTraining start time:')\n","\n","trainer.run_training()\n","\n","log_time('\\tTraining end time:')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcRbmTmhEQXR"},"source":["print('4 - validate model')\n","validator = Ham10000ResNet18Validator(model, validation_dataloader)\n","validator.run_validation()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdMgDxnoEWCd"},"source":["print('5 - make predictions')\n","predictor = Ham10000ResNet18Predictor(model, test_dataloader)\n","predictor.run_predictor()"],"execution_count":null,"outputs":[]}]}